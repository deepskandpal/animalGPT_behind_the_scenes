{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a187da-0eca-498a-8635-ac943e3fffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b2361e-1a7c-4982-b2b6-3c739e5ad482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34094a98-8bf9-427d-8081-caa93a8f969b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f720545b-3b66-49b4-be0e-ab7551b5a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa58accf4f04ec28545202cf9ef0c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea17cf-98ee-4306-81f5-5e0e4722c613",
   "metadata": {},
   "source": [
    "# Generate dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61566bf-b8fd-4503-82f0-e6a7e7f593cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Dummy dataset generation for film reviews\n",
    "def generate_film_reviews(num_reviews=100, max_review_length=200):\n",
    "    positive_adjectives = [\"amazing\", \"fantastic\", \"captivating\", \"outstanding\", \"excellent\"]\n",
    "    negative_adjectives = [\"disappointing\", \"boring\", \"predictable\", \"mediocre\", \"unimpressive\"]\n",
    "    movies = [\"The Midnight Star\", \"Dreamscape\", \"Eternal Echo\", \"Lost Horizon\", \"Whispering Shadows\"]\n",
    "\n",
    "    reviews = []\n",
    "\n",
    "    for _ in range(num_reviews):\n",
    "        movie = random.choice(movies)\n",
    "        rating = random.randint(1, 10)\n",
    "        adjective = random.choice(positive_adjectives) if rating > 5 else random.choice(negative_adjectives)\n",
    "        review_text = f\"{movie} is {adjective}! I would give it a {rating}/10. \"\n",
    "        review_text += \" \".join([\"This movie\", \"exceeded my expectations.\", \"Highly recommended!\", \"A must-watch!\"] * random.randint(1, 3))\n",
    "        review_text += \"\\n\\n\"\n",
    "\n",
    "        # Trim the review if it exceeds the max length\n",
    "        review_text = review_text[:max_review_length]\n",
    "\n",
    "        reviews.append(review_text)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Save the dummy dataset to a file\n",
    "dummy_reviews = generate_film_reviews()\n",
    "dataset_path = \"film_reviews_dataset.txt\"\n",
    "\n",
    "with open(dataset_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(dummy_reviews)\n",
    "\n",
    "print(f\"Dummy dataset saved to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7248d2a-57ff-4cfa-86ba-cdbf88375368",
   "metadata": {},
   "source": [
    "# Train GPT2 model on dummy txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc4844-589c-4b48-868a-754db3b41f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Load your custom film reviews dataset\n",
    "dataset_path = \"./movie_reviews_dataset/reviews_1.txt\"\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=dataset_path,\n",
    "    block_size=128,  # Adjust the block size according to your dataset\n",
    ")\n",
    "\n",
    "# Use the default data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./film_reviews_fine_tuned_v2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./film_reviews_fine_tuned_v2\")\n",
    "tokenizer.save_pretrained(\"./film_reviews_fine_tuned_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af09552-f7b0-4d75-bfb2-8d78934118e8",
   "metadata": {},
   "source": [
    "# Train GPT2 on proper datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d867dc6-6d02-45f1-8c3f-47457d90667e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe009d-22ef-42a3-b2fc-c381146794eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "dataset_path = \"./datasets/All data_1.txt\"\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=dataset_path,\n",
    "    block_size=128,  # Adjust the block size according to your dataset\n",
    ")\n",
    "\n",
    "# Use the default data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# *    num_train_epochs=3,              # total number of training epochs*\n",
    "# *    per_device_train_batch_size=16,  # batch size per device during training*\n",
    "# *    per_device_eval_batch_size=16,   # batch size for evaluation*\n",
    "# *    warmup_steps=50,                 # number of warmup steps for learning rate scheduler*\n",
    "# *    weight_decay=0.01,               # strength of weight decay*\n",
    "# *    logging_dir='./logs',            # directory for storing logs*\n",
    "# *    logging_steps=20,*\n",
    "# *    evaluation_strategy=\"steps\"*        \n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./anmialGPTV1\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=300,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',            # directory for storing logs*\n",
    ")\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.log_history:\n",
    "            print(f\"Step {state.global_step}, Loss: {state.log_history[-1]['loss']:.4f}\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Create Trainer instance with the custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./anmialGPTV1\")\n",
    "tokenizer.save_pretrained(\"./anmialGPTV1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a5ed0-0241-46eb-b864-61c86fb231f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4a3fe-e4b1-491c-be02-f795960477ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cd112-3b1d-4f85-af5b-7dd99995c3e6",
   "metadata": {},
   "source": [
    "# Prediction : Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07692190-acd5-4e0c-9a5b-db9e84647d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./film_reviews_fine_tuned_v1\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prompt for text generation\n",
    "prompt = \"The movie I watched yesterday was\"\n",
    "\n",
    "# Generate film review\n",
    "generated_review = text_generator(prompt, max_length=150, num_return_sequences=1, temperature=0.1)[0]['generated_text']\n",
    "\n",
    "# Print the generated review\n",
    "print(\"Generated Film Review:\")\n",
    "print(generated_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70294fa3-583d-4afa-a147-440aa0511d90",
   "metadata": {},
   "source": [
    "# Prediction : Conversation Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a23ad-1630-42a0-b39a-c0efb714be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "torch.manual_seed(100)\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./anmialGPTV1\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Start a conversation with the user\n",
    "print(\"Movie Bot: Hi there! Let's talk about Animal. You can type 'exit' to end the conversation.\")\n",
    "\n",
    "# while True:\n",
    "#     # Get user input\n",
    "#     user_input = input(\"You: \")\n",
    "\n",
    "#     # Check for exit condition\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         print(\"Movie Bot: Goodbye!\")\n",
    "#         break\n",
    "\n",
    "#     # Generate response\n",
    "#     generated_response = text_generator(user_input, top_k=10, top_p=0.99, max_length=150, num_return_sequences=3, temperature=0.9)[0]['generated_text']\n",
    "\n",
    "#     # Print the bot's response\n",
    "#     print(\"Movie Bot:\", generated_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957249d-19fa-45bc-b28d-3da6b25a2bbc",
   "metadata": {},
   "source": [
    "## Streamer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1489dcc-6612-4aac-b03b-ca8c02240ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3ee59e-17ed-4210-a38f-f0142a640a92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgeneration_config\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd272b-92e2-4ba8-b160-b42ca183ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Should filmmakers be held accountable\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b93a93-d339-4f6f-8055-4e8ff653489e",
   "metadata": {},
   "source": [
    "## Beam Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1487b-cef7-4db7-9eb7-f2f312f47bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8095b83-2372-4045-8373-f4dc5f4a1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6b5b0-47b3-4dbd-a603-ffda8a4987d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0bdf4-3d61-4cbc-a8b6-3cfd601146ca",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f4258-8b3b-4d7a-816f-fec7ebfa8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b57c9d-48f0-4562-b9dc-27cff92d9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69054f65-02e6-43a7-8e2b-8bb27a690b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b716ad2-c3ef-4498-b6ad-c3c988b9841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7579b-e5ba-45d0-ab38-33708c0d430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415e8ef-99df-46dd-a020-d49605b2dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0,   \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95655a4-4d19-4aee-ac4f-90ef025e01fc",
   "metadata": {},
   "source": [
    "# DIALOGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0dcdc1-ecec-413d-9646-23ce2e0c64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dfrom transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecb3ab-41e9-4368-82e3-38cdd7a12abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74de46-8959-4f68-9438-10e77baefdac",
   "metadata": {},
   "source": [
    "## Adding BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70026b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Configuration file exists at /Users/deepanshu.kandpal/Library/Application Support/pypoetry, reusing this directory.\\n\\nConsider moving TOML configuration files to /Users/deepanshu.kandpal/Library/Preferences/pypoetry, as support for the legacy directory will be removed in an upcoming release.',)\n",
      "\u001b[33mThe currently activated Python version 3.8.18 is not supported by the project (^3.9,<3.12).\n",
      "Trying to find and use a compatible version.\u001b[39m \n",
      "Using \u001b[36mpython3\u001b[39m (3.9.6)\n",
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mpython-dotenv\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Configuration file exists at /Users/deepanshu.kandpal/Library/Application Support/pypoetry, reusing this directory.\\n\\nConsider moving TOML configuration files to /Users/deepanshu.kandpal/Library/Preferences/pypoetry, as support for the legacy directory will be removed in an upcoming release.',)\n",
      "\u001b[33mThe currently activated Python version 3.8.18 is not supported by the project (^3.9,<3.12).\n",
      "Trying to find and use a compatible version.\u001b[39m \n",
      "Using \u001b[36mpython3\u001b[39m (3.9.6)\n",
      "\u001b[36maccelerate                   \u001b[39m \u001b[39;1m0.25.0      \u001b[39;22m Accelerate\n",
      "\u001b[36maiohttp                      \u001b[39m \u001b[39;1m3.9.1       \u001b[39;22m Async http client/server framewo...\n",
      "\u001b[36maiosignal                    \u001b[39m \u001b[39;1m1.3.1       \u001b[39;22m aiosignal: a list of registered ...\n",
      "\u001b[36malembic                      \u001b[39m \u001b[39;1m1.13.0      \u001b[39;22m A database migration tool for SQ...\n",
      "\u001b[36mantlr4-python3-runtime       \u001b[39m \u001b[39;1m4.9.3       \u001b[39;22m ANTLR 4.9.3 runtime for Python 3.7\n",
      "\u001b[36manyio                        \u001b[39m \u001b[39;1m4.1.0       \u001b[39;22m High level compatibility layer f...\n",
      "\u001b[36mappnope                      \u001b[39m \u001b[39;1m0.1.3       \u001b[39;22m Disable App Nap on macOS >= 10.9\n",
      "\u001b[36margon2-cffi                  \u001b[39m \u001b[39;1m23.1.0      \u001b[39;22m Argon2 for Python\n",
      "\u001b[36margon2-cffi-bindings         \u001b[39m \u001b[39;1m21.2.0      \u001b[39;22m Low-level CFFI bindings for Argon2\n",
      "\u001b[36marrow                        \u001b[39m \u001b[39;1m1.3.0       \u001b[39;22m Better dates & times for Python\n",
      "\u001b[36masteroid-filterbanks         \u001b[39m \u001b[39;1m0.4.0       \u001b[39;22m Asteroid's filterbanks\n",
      "\u001b[36masttokens                    \u001b[39m \u001b[39;1m2.4.1       \u001b[39;22m Annotate AST trees with source c...\n",
      "\u001b[36masync-lru                    \u001b[39m \u001b[39;1m2.0.4       \u001b[39;22m Simple LRU cache for asyncio\n",
      "\u001b[36masync-timeout                \u001b[39m \u001b[39;1m4.0.3       \u001b[39;22m Timeout context manager for asyn...\n",
      "\u001b[36mattrs                        \u001b[39m \u001b[39;1m23.1.0      \u001b[39;22m Classes Without Boilerplate\n",
      "\u001b[36maudioread                    \u001b[39m \u001b[39;1m3.0.1       \u001b[39;22m Multi-library, cross-platform au...\n",
      "\u001b[36mbabel                        \u001b[39m \u001b[39;1m2.13.1      \u001b[39;22m Internationalization utilities\n",
      "\u001b[36mbeautifulsoup4               \u001b[39m \u001b[39;1m4.12.2      \u001b[39;22m Screen-scraping library\n",
      "\u001b[36mbleach                       \u001b[39m \u001b[39;1m6.1.0       \u001b[39;22m An easy safelist-based HTML-sani...\n",
      "\u001b[36mblinker                      \u001b[39m \u001b[39;1m1.7.0       \u001b[39;22m Fast, simple object-to-object an...\n",
      "\u001b[36mcachetools                   \u001b[39m \u001b[39;1m5.3.2       \u001b[39;22m Extensible memoizing collections...\n",
      "\u001b[36mcertifi                      \u001b[39m \u001b[39;1m2023.11.17  \u001b[39;22m Python package for providing Moz...\n",
      "\u001b[36mcffi                         \u001b[39m \u001b[39;1m1.16.0      \u001b[39;22m Foreign Function Interface for P...\n",
      "\u001b[36mcharset-normalizer           \u001b[39m \u001b[39;1m3.3.2       \u001b[39;22m The Real First Universal Charset...\n",
      "\u001b[36mclick                        \u001b[39m \u001b[39;1m8.1.7       \u001b[39;22m Composable command line interfac...\n",
      "\u001b[36mcolorama                     \u001b[39m \u001b[39;1m0.4.6       \u001b[39;22m Cross-platform colored terminal ...\n",
      "\u001b[36mcolorlog                     \u001b[39m \u001b[39;1m6.8.0       \u001b[39;22m Add colours to the output of Pyt...\n",
      "\u001b[36mcomm                         \u001b[39m \u001b[39;1m0.2.0       \u001b[39;22m Jupyter Python Comm implementati...\n",
      "\u001b[36mcontourpy                    \u001b[39m \u001b[39;1m1.2.0       \u001b[39;22m Python library for calculating c...\n",
      "\u001b[36mcycler                       \u001b[39m \u001b[39;1m0.12.1      \u001b[39;22m Composable style cycles\n",
      "\u001b[36mcython                       \u001b[39m \u001b[39;1m3.0.7       \u001b[39;22m The Cython compiler for writing ...\n",
      "\u001b[31mdb-dtypes                    \u001b[39m \u001b[39;1m1.2.0       \u001b[39;22m Pandas Data Types for SQL system...\n",
      "\u001b[36mdebugpy                      \u001b[39m \u001b[39;1m1.8.0       \u001b[39;22m An implementation of the Debug A...\n",
      "\u001b[36mdecorator                    \u001b[39m \u001b[39;1m5.1.1       \u001b[39;22m Decorators for Humans\n",
      "\u001b[36mdefusedxml                   \u001b[39m \u001b[39;1m0.7.1       \u001b[39;22m XML bomb protection for Python s...\n",
      "\u001b[36mdocopt                       \u001b[39m \u001b[39;1m0.6.2       \u001b[39;22m Pythonic argument parser, that w...\n",
      "\u001b[36meinops                       \u001b[39m \u001b[39;1m0.7.0       \u001b[39;22m A new flavour of deep learning o...\n",
      "\u001b[36mexceptiongroup               \u001b[39m \u001b[39;1m1.2.0       \u001b[39;22m Backport of PEP 654 (exception g...\n",
      "\u001b[36mexecuting                    \u001b[39m \u001b[39;1m2.0.1       \u001b[39;22m Get the currently executing AST ...\n",
      "\u001b[36mfaiss-cpu                    \u001b[39m \u001b[39;1m1.7.4       \u001b[39;22m A library for efficient similari...\n",
      "\u001b[36mfastjsonschema               \u001b[39m \u001b[39;1m2.19.0      \u001b[39;22m Fastest Python implementation of...\n",
      "\u001b[36mfilelock                     \u001b[39m \u001b[39;1m3.13.1      \u001b[39;22m A platform independent file lock.\n",
      "\u001b[36mflask                        \u001b[39m \u001b[39;1m3.0.0       \u001b[39;22m A simple framework for building ...\n",
      "\u001b[36mfonttools                    \u001b[39m \u001b[39;1m4.46.0      \u001b[39;22m Tools to manipulate font files\n",
      "\u001b[36mfqdn                         \u001b[39m \u001b[39;1m1.5.1       \u001b[39;22m Validates fully-qualified domain...\n",
      "\u001b[36mfrozenlist                   \u001b[39m \u001b[39;1m1.4.0       \u001b[39;22m A list-like structure which impl...\n",
      "\u001b[36mfsspec                       \u001b[39m \u001b[39;1m2023.12.1   \u001b[39;22m File-system specification\n",
      "\u001b[36mgoogle-api-core              \u001b[39m \u001b[39;1m2.15.0      \u001b[39;22m Google API client core library\n",
      "\u001b[36mgoogle-api-python-client     \u001b[39m \u001b[39;1m2.115.0     \u001b[39;22m Google API Client Library for Py...\n",
      "\u001b[36mgoogle-auth                  \u001b[39m \u001b[39;1m2.25.2      \u001b[39;22m Google Authentication Library\n",
      "\u001b[36mgoogle-auth-httplib2         \u001b[39m \u001b[39;1m0.1.1       \u001b[39;22m Google Authentication Library: h...\n",
      "\u001b[31mgoogle-cloud-bigquery        \u001b[39m \u001b[39;1m3.17.1      \u001b[39;22m Google BigQuery API client library\n",
      "\u001b[36mgoogle-cloud-bigquery-storage\u001b[39m \u001b[39;1m2.24.0      \u001b[39;22m Google Cloud Bigquery Storage AP...\n",
      "\u001b[31mgoogle-cloud-core            \u001b[39m \u001b[39;1m2.4.1       \u001b[39;22m Google Cloud API client core lib...\n",
      "\u001b[36mgoogle-cloud-speech          \u001b[39m \u001b[39;1m2.23.0      \u001b[39;22m Google Cloud Speech API client l...\n",
      "\u001b[31mgoogle-crc32c                \u001b[39m \u001b[39;1m1.5.0       \u001b[39;22m A python wrapper of the C librar...\n",
      "\u001b[31mgoogle-resumable-media       \u001b[39m \u001b[39;1m2.7.0       \u001b[39;22m Utilities for Google Media Downl...\n",
      "\u001b[36mgoogleapis-common-protos     \u001b[39m \u001b[39;1m1.62.0      \u001b[39;22m Common protobufs used in Google ...\n",
      "\u001b[36mgreenlet                     \u001b[39m \u001b[39;1m3.0.2       \u001b[39;22m Lightweight in-process concurren...\n",
      "\u001b[36mgrpcio                       \u001b[39m \u001b[39;1m1.60.0      \u001b[39;22m HTTP/2-based RPC framework\n",
      "\u001b[36mgrpcio-status                \u001b[39m \u001b[39;1m1.60.0      \u001b[39;22m Status proto mapping for gRPC\n",
      "\u001b[36mhttplib2                     \u001b[39m \u001b[39;1m0.22.0      \u001b[39;22m A comprehensive HTTP client libr...\n",
      "\u001b[36mhuggingface-hub              \u001b[39m \u001b[39;1m0.19.4      \u001b[39;22m Client library to download and p...\n",
      "\u001b[36mhyperpyyaml                  \u001b[39m \u001b[39;1m1.2.2       \u001b[39;22m Extensions to YAML syntax for be...\n",
      "\u001b[36midna                         \u001b[39m \u001b[39;1m3.6         \u001b[39;22m Internationalized Domain Names i...\n",
      "\u001b[36mimportlib-metadata           \u001b[39m \u001b[39;1m7.0.0       \u001b[39;22m Read metadata from Python packages\n",
      "\u001b[36mimportlib-resources          \u001b[39m \u001b[39;1m6.1.1       \u001b[39;22m Read resources from Python packages\n",
      "\u001b[36mipykernel                    \u001b[39m \u001b[39;1m6.27.1      \u001b[39;22m IPython Kernel for Jupyter\n",
      "\u001b[36mipython                      \u001b[39m \u001b[39;1m8.18.1      \u001b[39;22m IPython: Productive Interactive ...\n",
      "\u001b[36mipywidgets                   \u001b[39m \u001b[39;1m8.1.1       \u001b[39;22m Jupyter interactive widgets\n",
      "\u001b[36misoduration                  \u001b[39m \u001b[39;1m20.11.0     \u001b[39;22m Operations with ISO 8601 durations\n",
      "\u001b[36mitsdangerous                 \u001b[39m \u001b[39;1m2.1.2       \u001b[39;22m Safely pass data to untrusted en...\n",
      "\u001b[36mjedi                         \u001b[39m \u001b[39;1m0.19.1      \u001b[39;22m An autocompletion tool for Pytho...\n",
      "\u001b[36mjinja2                       \u001b[39m \u001b[39;1m3.1.2       \u001b[39;22m A very fast and expressive templ...\n",
      "\u001b[36mjoblib                       \u001b[39m \u001b[39;1m1.3.2       \u001b[39;22m Lightweight pipelining with Pyth...\n",
      "\u001b[36mjson5                        \u001b[39m \u001b[39;1m0.9.14      \u001b[39;22m A Python implementation of the J...\n",
      "\u001b[36mjsonpointer                  \u001b[39m \u001b[39;1m2.4         \u001b[39;22m Identify specific nodes in a JSO...\n",
      "\u001b[36mjsonschema                   \u001b[39m \u001b[39;1m4.20.0      \u001b[39;22m An implementation of JSON Schema...\n",
      "\u001b[36mjsonschema-specifications    \u001b[39m \u001b[39;1m2023.11.2   \u001b[39;22m The JSON Schema meta-schemas and...\n",
      "\u001b[36mjulius                       \u001b[39m \u001b[39;1m0.2.7       \u001b[39;22m Nice DSP sweets: resampling, FFT...\n",
      "\u001b[36mjupyter-client               \u001b[39m \u001b[39;1m8.6.0       \u001b[39;22m Jupyter protocol implementation ...\n",
      "\u001b[36mjupyter-core                 \u001b[39m \u001b[39;1m5.5.0       \u001b[39;22m Jupyter core package. A base pac...\n",
      "\u001b[36mjupyter-events               \u001b[39m \u001b[39;1m0.9.0       \u001b[39;22m Jupyter Event System library\n",
      "\u001b[36mjupyter-lsp                  \u001b[39m \u001b[39;1m2.2.1       \u001b[39;22m Multi-Language Server WebSocket ...\n",
      "\u001b[36mjupyter-server               \u001b[39m \u001b[39;1m2.12.1      \u001b[39;22m The backend—i.e. core services, ...\n",
      "\u001b[36mjupyter-server-terminals     \u001b[39m \u001b[39;1m0.4.4       \u001b[39;22m A Jupyter Server Extension Provi...\n",
      "\u001b[36mjupyterlab                   \u001b[39m \u001b[39;1m4.0.9       \u001b[39;22m JupyterLab computational environ...\n",
      "\u001b[36mjupyterlab-pygments          \u001b[39m \u001b[39;1m0.3.0       \u001b[39;22m Pygments theme using JupyterLab ...\n",
      "\u001b[36mjupyterlab-server            \u001b[39m \u001b[39;1m2.25.2      \u001b[39;22m A set of server components for J...\n",
      "\u001b[36mjupyterlab-widgets           \u001b[39m \u001b[39;1m3.0.9       \u001b[39;22m Jupyter interactive widgets for ...\n",
      "\u001b[36mkiwisolver                   \u001b[39m \u001b[39;1m1.4.5       \u001b[39;22m A fast implementation of the Cas...\n",
      "\u001b[36mlazy-loader                  \u001b[39m \u001b[39;1m0.3         \u001b[39;22m lazy_loader\n",
      "\u001b[36mlibrosa                      \u001b[39m \u001b[39;1m0.10.1      \u001b[39;22m Python module for audio and musi...\n",
      "\u001b[36mlightning                    \u001b[39m \u001b[39;1m2.1.2       \u001b[39;22m The Deep Learning framework to t...\n",
      "\u001b[36mlightning-utilities          \u001b[39m \u001b[39;1m0.10.0      \u001b[39;22m PyTorch Lightning Sample project.\n",
      "\u001b[36mllvmlite                     \u001b[39m \u001b[39;1m0.41.1      \u001b[39;22m lightweight wrapper around basic...\n",
      "\u001b[36mmako                         \u001b[39m \u001b[39;1m1.3.0       \u001b[39;22m A super-fast templating language...\n",
      "\u001b[36mmarkdown-it-py               \u001b[39m \u001b[39;1m3.0.0       \u001b[39;22m Python port of markdown-it. Mark...\n",
      "\u001b[36mmarkupsafe                   \u001b[39m \u001b[39;1m2.1.3       \u001b[39;22m Safely add untrusted strings to ...\n",
      "\u001b[36mmatplotlib                   \u001b[39m \u001b[39;1m3.8.2       \u001b[39;22m Python plotting package\n",
      "\u001b[36mmatplotlib-inline            \u001b[39m \u001b[39;1m0.1.6       \u001b[39;22m Inline Matplotlib backend for Ju...\n",
      "\u001b[36mmdurl                        \u001b[39m \u001b[39;1m0.1.2       \u001b[39;22m Markdown URL utilities\n",
      "\u001b[36mmistune                      \u001b[39m \u001b[39;1m3.0.2       \u001b[39;22m A sane and fast Markdown parser ...\n",
      "\u001b[36mmpmath                       \u001b[39m \u001b[39;1m1.3.0       \u001b[39;22m Python library for arbitrary-pre...\n",
      "\u001b[36mmsgpack                      \u001b[39m \u001b[39;1m1.0.7       \u001b[39;22m MessagePack serializer\n",
      "\u001b[36mmultidict                    \u001b[39m \u001b[39;1m6.0.4       \u001b[39;22m multidict implementation\n",
      "\u001b[36mnbclient                     \u001b[39m \u001b[39;1m0.9.0       \u001b[39;22m A client library for executing n...\n",
      "\u001b[36mnbconvert                    \u001b[39m \u001b[39;1m7.12.0      \u001b[39;22m Converting Jupyter Notebooks\n",
      "\u001b[36mnbformat                     \u001b[39m \u001b[39;1m5.9.2       \u001b[39;22m The Jupyter Notebook format\n",
      "\u001b[36mnest-asyncio                 \u001b[39m \u001b[39;1m1.5.8       \u001b[39;22m Patch asyncio to allow nested ev...\n",
      "\u001b[36mnetworkx                     \u001b[39m \u001b[39;1m3.2.1       \u001b[39;22m Python package for creating and ...\n",
      "\u001b[36mnltk                         \u001b[39m \u001b[39;1m3.8.1       \u001b[39;22m Natural Language Toolkit\n",
      "\u001b[36mnotebook-shim                \u001b[39m \u001b[39;1m0.2.3       \u001b[39;22m A shim layer for notebook traits...\n",
      "\u001b[36mnumba                        \u001b[39m \u001b[39;1m0.58.1      \u001b[39;22m compiling Python code using LLVM\n",
      "\u001b[36mnumpy                        \u001b[39m \u001b[39;1m1.26.2      \u001b[39;22m Fundamental package for array co...\n",
      "\u001b[36momegaconf                    \u001b[39m \u001b[39;1m2.3.0       \u001b[39;22m A flexible configuration library\n",
      "\u001b[36moptuna                       \u001b[39m \u001b[39;1m3.5.0       \u001b[39;22m A hyperparameter optimization fr...\n",
      "\u001b[36moverrides                    \u001b[39m \u001b[39;1m7.4.0       \u001b[39;22m A decorator to automatically det...\n",
      "\u001b[36mpackaging                    \u001b[39m \u001b[39;1m23.2        \u001b[39;22m Core utilities for Python packages\n",
      "\u001b[36mpandas                       \u001b[39m \u001b[39;1m2.1.4       \u001b[39;22m Powerful data structures for dat...\n",
      "\u001b[36mpandocfilters                \u001b[39m \u001b[39;1m1.5.0       \u001b[39;22m Utilities for writing pandoc fil...\n",
      "\u001b[36mparso                        \u001b[39m \u001b[39;1m0.8.3       \u001b[39;22m A Python Parser\n",
      "\u001b[36mpexpect                      \u001b[39m \u001b[39;1m4.9.0       \u001b[39;22m Pexpect allows easy control of i...\n",
      "\u001b[36mpillow                       \u001b[39m \u001b[39;1m10.1.0      \u001b[39;22m Python Imaging Library (Fork)\n",
      "\u001b[36mplatformdirs                 \u001b[39m \u001b[39;1m4.1.0       \u001b[39;22m A small Python package for deter...\n",
      "\u001b[36mpooch                        \u001b[39m \u001b[39;1m1.8.0       \u001b[39;22m \"Pooch manages your Python libra...\n",
      "\u001b[36mprimepy                      \u001b[39m \u001b[39;1m1.3         \u001b[39;22m This module contains several use...\n",
      "\u001b[36mprometheus-client            \u001b[39m \u001b[39;1m0.19.0      \u001b[39;22m Python client for the Prometheus...\n",
      "\u001b[36mprompt-toolkit               \u001b[39m \u001b[39;1m3.0.41      \u001b[39;22m Library for building powerful in...\n",
      "\u001b[36mproto-plus                   \u001b[39m \u001b[39;1m1.23.0      \u001b[39;22m Beautiful, Pythonic protocol buf...\n",
      "\u001b[36mprotobuf                     \u001b[39m \u001b[39;1m4.25.1      \u001b[39;22m \n",
      "\u001b[36mpsutil                       \u001b[39m \u001b[39;1m5.9.6       \u001b[39;22m Cross-platform lib for process a...\n",
      "\u001b[36mptyprocess                   \u001b[39m \u001b[39;1m0.7.0       \u001b[39;22m Run a subprocess in a pseudo ter...\n",
      "\u001b[36mpure-eval                    \u001b[39m \u001b[39;1m0.2.2       \u001b[39;22m Safely evaluate AST nodes withou...\n",
      "\u001b[36mpyannote-audio               \u001b[39m \u001b[39;1m3.1.1       \u001b[39;22m Neural building blocks for speak...\n",
      "\u001b[36mpyannote-core                \u001b[39m \u001b[39;1m5.0.0       \u001b[39;22m Advanced data structures for han...\n",
      "\u001b[36mpyannote-database            \u001b[39m \u001b[39;1m5.0.1       \u001b[39;22m Interface to multimedia database...\n",
      "\u001b[36mpyannote-metrics             \u001b[39m \u001b[39;1m3.2.1       \u001b[39;22m a toolkit for reproducible evalu...\n",
      "\u001b[36mpyannote-pipeline            \u001b[39m \u001b[39;1m3.0.1       \u001b[39;22m Tunable pipelines.\n",
      "\u001b[31mpyarrow                      \u001b[39m \u001b[39;1m15.0.0      \u001b[39;22m Python library for Apache Arrow\n",
      "\u001b[36mpyasn1                       \u001b[39m \u001b[39;1m0.5.1       \u001b[39;22m Pure-Python implementation of AS...\n",
      "\u001b[36mpyasn1-modules               \u001b[39m \u001b[39;1m0.3.0       \u001b[39;22m A collection of ASN.1-based prot...\n",
      "\u001b[36mpycparser                    \u001b[39m \u001b[39;1m2.21        \u001b[39;22m C parser in Python\n",
      "\u001b[36mpydub                        \u001b[39m \u001b[39;1m0.25.1      \u001b[39;22m Manipulate audio with an simple ...\n",
      "\u001b[36mpygments                     \u001b[39m \u001b[39;1m2.17.2      \u001b[39;22m Pygments is a syntax highlightin...\n",
      "\u001b[36mpyparsing                    \u001b[39m \u001b[39;1m3.1.1       \u001b[39;22m pyparsing module - Classes and m...\n",
      "\u001b[36mpython-dateutil              \u001b[39m \u001b[39;1m2.8.2       \u001b[39;22m Extensions to the standard Pytho...\n",
      "\u001b[36mpython-dotenv                \u001b[39m \u001b[39;1m1.0.1       \u001b[39;22m Read key-value pairs from a .env...\n",
      "\u001b[36mpython-json-logger           \u001b[39m \u001b[39;1m2.0.7       \u001b[39;22m A python library adding a json l...\n",
      "\u001b[36mpytorch-lightning            \u001b[39m \u001b[39;1m2.1.2       \u001b[39;22m PyTorch Lightning is the lightwe...\n",
      "\u001b[36mpytorch-metric-learning      \u001b[39m \u001b[39;1m2.3.0       \u001b[39;22m The easiest way to use deep metr...\n",
      "\u001b[36mpytube                       \u001b[39m \u001b[39;1m15.0.0      \u001b[39;22m Python 3 library for downloading...\n",
      "\u001b[36mpytz                         \u001b[39m \u001b[39;1m2023.3.post1\u001b[39;22m World timezone definitions, mode...\n",
      "\u001b[36mpyyaml                       \u001b[39m \u001b[39;1m6.0.1       \u001b[39;22m YAML parser and emitter for Python\n",
      "\u001b[36mpyzmq                        \u001b[39m \u001b[39;1m25.1.2      \u001b[39;22m Python bindings for 0MQ\n",
      "\u001b[36mreferencing                  \u001b[39m \u001b[39;1m0.32.0      \u001b[39;22m JSON Referencing + Python\n",
      "\u001b[36mregex                        \u001b[39m \u001b[39;1m2023.10.3   \u001b[39;22m Alternative regular expression m...\n",
      "\u001b[36mrequests                     \u001b[39m \u001b[39;1m2.31.0      \u001b[39;22m Python HTTP for Humans.\n",
      "\u001b[36mrfc3339-validator            \u001b[39m \u001b[39;1m0.1.4       \u001b[39;22m A pure python RFC3339 validator\n",
      "\u001b[36mrfc3986-validator            \u001b[39m \u001b[39;1m0.1.1       \u001b[39;22m Pure python rfc3986 validator\n",
      "\u001b[36mrich                         \u001b[39m \u001b[39;1m13.7.0      \u001b[39;22m Render rich text, tables, progre...\n",
      "\u001b[36mrpds-py                      \u001b[39m \u001b[39;1m0.13.2      \u001b[39;22m Python bindings to Rust's persis...\n",
      "\u001b[36mrsa                          \u001b[39m \u001b[39;1m4.9         \u001b[39;22m Pure-Python RSA implementation\n",
      "\u001b[36mruamel-yaml                  \u001b[39m \u001b[39;1m0.18.5      \u001b[39;22m ruamel.yaml is a YAML parser/emi...\n",
      "\u001b[36mruamel-yaml-clib             \u001b[39m \u001b[39;1m0.2.8       \u001b[39;22m C version of reader, parser and ...\n",
      "\u001b[36msafetensors                  \u001b[39m \u001b[39;1m0.4.1       \u001b[39;22m \n",
      "\u001b[36mscikit-learn                 \u001b[39m \u001b[39;1m1.3.2       \u001b[39;22m A set of python modules for mach...\n",
      "\u001b[36mscipy                        \u001b[39m \u001b[39;1m1.11.4      \u001b[39;22m Fundamental algorithms for scien...\n",
      "\u001b[36msemver                       \u001b[39m \u001b[39;1m3.0.2       \u001b[39;22m Python helper for Semantic Versi...\n",
      "\u001b[36msend2trash                   \u001b[39m \u001b[39;1m1.8.2       \u001b[39;22m Send file to trash natively unde...\n",
      "\u001b[36msentence-transformers        \u001b[39m \u001b[39;1m2.2.2       \u001b[39;22m Multilingual text embeddings\n",
      "\u001b[36msentencepiece                \u001b[39m \u001b[39;1m0.1.99      \u001b[39;22m SentencePiece python wrapper\n",
      "\u001b[36msetuptools                   \u001b[39m \u001b[39;1m69.0.2      \u001b[39;22m Easily download, build, install,...\n",
      "\u001b[36mshellingham                  \u001b[39m \u001b[39;1m1.5.4       \u001b[39;22m Tool to Detect Surrounding Shell\n",
      "\u001b[36msix                          \u001b[39m \u001b[39;1m1.16.0      \u001b[39;22m Python 2 and 3 compatibility uti...\n",
      "\u001b[36msniffio                      \u001b[39m \u001b[39;1m1.3.0       \u001b[39;22m Sniff out which async library yo...\n",
      "\u001b[36msortedcontainers             \u001b[39m \u001b[39;1m2.4.0       \u001b[39;22m Sorted Containers -- Sorted List...\n",
      "\u001b[36msoundfile                    \u001b[39m \u001b[39;1m0.12.1      \u001b[39;22m An audio library based on libsnd...\n",
      "\u001b[36msoupsieve                    \u001b[39m \u001b[39;1m2.5         \u001b[39;22m A modern CSS selector implementa...\n",
      "\u001b[36msoxr                         \u001b[39m \u001b[39;1m0.3.7       \u001b[39;22m High quality, one-dimensional sa...\n",
      "\u001b[36mspeechbrain                  \u001b[39m \u001b[39;1m0.5.16      \u001b[39;22m All-in-one speech toolkit in pur...\n",
      "\u001b[36mspeechrecognition            \u001b[39m \u001b[39;1m3.10.1      \u001b[39;22m Library for performing speech re...\n",
      "\u001b[36msqlalchemy                   \u001b[39m \u001b[39;1m2.0.23      \u001b[39;22m Database Abstraction Library\n",
      "\u001b[36mstack-data                   \u001b[39m \u001b[39;1m0.6.3       \u001b[39;22m Extract data from python stack f...\n",
      "\u001b[36msympy                        \u001b[39m \u001b[39;1m1.12        \u001b[39;22m Computer algebra system (CAS) in...\n",
      "\u001b[36mtabulate                     \u001b[39m \u001b[39;1m0.9.0       \u001b[39;22m Pretty-print tabular data\n",
      "\u001b[36mtensorboardx                 \u001b[39m \u001b[39;1m2.6.2.2     \u001b[39;22m TensorBoardX lets you watch Tens...\n",
      "\u001b[36mterminado                    \u001b[39m \u001b[39;1m0.18.0      \u001b[39;22m Tornado websocket backend for th...\n",
      "\u001b[36mthreadpoolctl                \u001b[39m \u001b[39;1m3.2.0       \u001b[39;22m threadpoolctl\n",
      "\u001b[36mtinycss2                     \u001b[39m \u001b[39;1m1.2.1       \u001b[39;22m A tiny CSS parser\n",
      "\u001b[36mtokenizers                   \u001b[39m \u001b[39;1m0.15.0      \u001b[39;22m \n",
      "\u001b[36mtomli                        \u001b[39m \u001b[39;1m2.0.1       \u001b[39;22m A lil' TOML parser\n",
      "\u001b[36mtorch                        \u001b[39m \u001b[39;1m2.1.1       \u001b[39;22m Tensors and Dynamic neural netwo...\n",
      "\u001b[36mtorch-audiomentations        \u001b[39m \u001b[39;1m0.11.0      \u001b[39;22m A Pytorch library for audio data...\n",
      "\u001b[36mtorch-pitch-shift            \u001b[39m \u001b[39;1m1.2.4       \u001b[39;22m \n",
      "\u001b[36mtorchaudio                   \u001b[39m \u001b[39;1m2.1.1       \u001b[39;22m An audio package for PyTorch\n",
      "\u001b[36mtorchmetrics                 \u001b[39m \u001b[39;1m1.2.1       \u001b[39;22m PyTorch native Metrics\n",
      "\u001b[36mtorchvision                  \u001b[39m \u001b[39;1m0.16.1      \u001b[39;22m image and video datasets and mod...\n",
      "\u001b[36mtornado                      \u001b[39m \u001b[39;1m6.4         \u001b[39;22m Tornado is a Python web framewor...\n",
      "\u001b[36mtqdm                         \u001b[39m \u001b[39;1m4.66.1      \u001b[39;22m Fast, Extensible Progress Meter\n",
      "\u001b[36mtraitlets                    \u001b[39m \u001b[39;1m5.14.0      \u001b[39;22m Traitlets Python configuration s...\n",
      "\u001b[36mtransformers                 \u001b[39m \u001b[39;1m4.35.2      \u001b[39;22m State-of-the-art Machine Learnin...\n",
      "\u001b[36mtyper                        \u001b[39m \u001b[39;1m0.9.0       \u001b[39;22m Typer, build great CLIs. Easy to...\n",
      "\u001b[36mtypes-python-dateutil        \u001b[39m \u001b[39;1m2.8.19.14   \u001b[39;22m Typing stubs for python-dateutil\n",
      "\u001b[36mtyping-extensions            \u001b[39m \u001b[39;1m4.8.0       \u001b[39;22m Backported and Experimental Type...\n",
      "\u001b[36mtzdata                       \u001b[39m \u001b[39;1m2023.3      \u001b[39;22m Provider of IANA time zone data\n",
      "\u001b[36muri-template                 \u001b[39m \u001b[39;1m1.3.0       \u001b[39;22m RFC 6570 URI Template Processor\n",
      "\u001b[36muritemplate                  \u001b[39m \u001b[39;1m4.1.1       \u001b[39;22m Implementation of RFC 6570 URI T...\n",
      "\u001b[36murllib3                      \u001b[39m \u001b[39;1m2.1.0       \u001b[39;22m HTTP library with thread-safe co...\n",
      "\u001b[36mwcwidth                      \u001b[39m \u001b[39;1m0.2.12      \u001b[39;22m Measures the displayed width of ...\n",
      "\u001b[36mwebcolors                    \u001b[39m \u001b[39;1m1.13        \u001b[39;22m A library for working with the c...\n",
      "\u001b[36mwebencodings                 \u001b[39m \u001b[39;1m0.5.1       \u001b[39;22m Character encoding aliases for l...\n",
      "\u001b[36mwebsocket-client             \u001b[39m \u001b[39;1m1.7.0       \u001b[39;22m WebSocket client for Python with...\n",
      "\u001b[36mwerkzeug                     \u001b[39m \u001b[39;1m3.0.1       \u001b[39;22m The comprehensive WSGI web appli...\n",
      "\u001b[36mwidgetsnbextension           \u001b[39m \u001b[39;1m4.0.9       \u001b[39;22m Jupyter interactive widgets for ...\n",
      "\u001b[36myarl                         \u001b[39m \u001b[39;1m1.9.4       \u001b[39;22m Yet another URL library\n",
      "\u001b[36mzipp                         \u001b[39m \u001b[39;1m3.17.0      \u001b[39;22m Backport of pathlib-compatible o...\n"
     ]
    }
   ],
   "source": [
    "!poetry show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27df0f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"animal.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b86584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: service_account\n",
      "project_id: placeapi-333910\n",
      "private_key_id: cba7da27e82d1a1abf43f648bb8ffa4bf504b415\n",
      "private_key: MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDT0Lf4+t2wMr7kDQTHLaxWeUTMNCDnkk1bqwgIGcsGJU4KXtNbS5uPkhCinh/y5KuiNLFqcIxdKl3ZdVWDrNqVeNno1c78bYhmSf7lql97VOBbZU4jIgaKPnc3DuCiKQR9FP1unqT/zV5413KYmRXRpiH7qdx4yy9sGq8Z4hJhhbQCPze016b2NOkhXypBnvT1U8ujhSDC+tCTLMYR+BNMzkuN3cdwbbTPTXlij9bdHOfDpZ1C71TIWOn9h+DEsultWAPptSHBvup22pl2c4MK2OmUSrLZxtp2hp7wYxNYhQpz29EcaOC9NdKjP2vcZucdbbIRTzewhsjMLTQC22LBAgMBAAECggEAC1TVIrq49V4hnC2eHdGxsIaYSZxYTksjXE4QBe7rT47bV+6uu+mVjt6viSOQbfwRuoG7JAiVWb6UereuNXcf0nM+pRgbFKH/dAltW3f716s4vWrz/Skq1GZ3Fjjzh3lORirDTBGsngqsjSZpeQRniGms07vIkaW9Cbl7uJTBivn1NdyiCVnLyRgnJ/cBcK9Ic0TPM8fi2AWPBNkeFFK3QNG37EzrTWuY0ygtTNs5jR0FbTsBru6FQz01AOeRSbrOvH7kgN3JsqTOCBj6YckUNuZSLltYi3Y+aKkpTg9eMLHDDD2vL7Bw8f7d4aZ8jUDHz0rxUX7PC1Qebwx2M6jkaQKBgQDtdmFcN//V0kWGOlrV+MEMz+EWdakuuTTuJYHrKk2F58O6sinDI1PkBqb5VuneUi4JC3QOvUtS1boTF0xjynOMbDPcf1EusIbNziGrefNpTfUlaP3Kj7elJmSi8EX4AReseVEnOvjISC8TAk3vWYkOGxeKn4UL3e88mU7FrmiucwKBgQDkWcm2P/VhEPR6spQSTJxmbTSbudPeLyDvP/LIZjiHllSNNLkoM3qefhlliaN1YNVGPX4DNXPNPxvT+HHyAM/nfzJkedlSGtIhRC0jhosNnlRH1/LiPixHWviDSTpxD0oLnVFQQrmBR1S9i+Mo/3vHxYfoU0JRosDknGMV+DhI+wKBgFAEiCJPLIKc7JiiqWFZBZSPqEWtOGlqN4/YS8M26hCpXKdYygHbsbUxGKsEwqIMBz89AMrGts8e/ijYvQETg4RaIc+3h1+4VHSddXMXERL3FhiZHIxZiYS5P0g4txsvIw8XgmU51gOHb3V0XVa+URTTxEcOIgMxsiA1ePncH6JJAoGBAKDpfLEG5P0fm4gS6zDx94xeDCJokpoRD95Ia+c30UIKd06CBrADhrbFGVRp51vBfsjecDwGoLHTdOKsQm9MjzUMdxBCpResesKibkSIfNh959ownKE9OqqVumWZZqthIMaKd1BdlKc7AsYHG246ipk94wWeFi+8xrzSTyieH9udAoGBANT8PlyWFRIGiQSKUwpw9GrfeeKfsLEorB9YsSLPa+lLIzpBmaeSPRMCAhT/vJ+BHnlJ5kXcqU8oMvDk62DNBLU8/+kURbLR9iDOnDyoavw+DLhB3FHInqh1pYZe4xpFVBD5c8m4aEJVcPdqWb+dDJ43dsgLaXNZw2w34CjXxy2w\n",
      "client_email: soai-dev-acn@placeapi-333910.iam.gserviceaccount.com\n",
      "client_id: 115497279223632701890\n",
      "auth_uri: https://accounts.google.com/o/oauth2/auth\n",
      "token_uri: https://oauth2.googleapis.com/token\n",
      "auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n",
      "client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/soai-dev-acn%40placeapi-333910.iam.gserviceaccount.com\n",
      "universe_domain: googleapis.com\n"
     ]
    }
   ],
   "source": [
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  animal_gpt.utils.config import (\n",
    "    default_bq_cliente_mail,\n",
    "    default_bq_private_key,\n",
    "    default_bq_token_uri,\n",
    "    default_project_id,\n",
    "    default_dialect,\n",
    "    default_private_key,\n",
    "    format_bqprivatekey,\n",
    ")\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqlogin = (\n",
    "        \"{\"\n",
    "        + '\"project_id\": \"'\n",
    "        + default_project_id\n",
    "        + '\", '\n",
    "        + '\"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\n'\n",
    "        + format_bqprivatekey(default_bq_private_key)\n",
    "        + '\\\\n-----END PRIVATE KEY-----\\\\n\", '\n",
    "        + '\"client_email\": \"'\n",
    "        + default_bq_cliente_mail\n",
    "        + '\", '\n",
    "        + '\"token_uri\": \"'\n",
    "        + default_bq_token_uri\n",
    "        + '\"'\n",
    "        + \"}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = json.loads(bqlogin)\n",
    "except Exception as e:\n",
    "    raise e(\"BQLOGIN must be a valid json string\")\n",
    "\n",
    "with open(default_private_key, \"wt\") as f:\n",
    "    f.write(bqlogin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e22418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(default_private_key, )\n",
    "client = bigquery.Client.from_service_account_json(default_private_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc24f4",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from flask import Flask, render_template, request\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyLogger:\n",
    "    def __init__(self, log_filename):\n",
    "        self.log_formatter = logging.Formatter('%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        self.log_handler = RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=5)\n",
    "        self.log_handler.setFormatter(self.log_formatter)\n",
    "        self.logger = logging.getLogger('my_logger')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.addHandler(self.log_handler)\n",
    "\n",
    "    def log_info(self, log_entry):\n",
    "        self.logger.info(log_entry)\n",
    "\n",
    "    def read_log_file(self, log_filename):\n",
    "        log_entries = []\n",
    "        with open(log_filename, 'r') as log_file:\n",
    "            for line in log_file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        log_entry = eval(line)\n",
    "                        log_entries.append(log_entry)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing log entry: {e}\")\n",
    "        return pd.DataFrame(log_entries)\n",
    "\n",
    "# Create an instance of MyLogger\n",
    "log_filename = f'logs/app_{time.strftime(\"%Y-%m-%d\")}.log'\n",
    "logger_instance = MyLogger(log_filename)\n",
    "\n",
    "act_id = str(uuid.uuid4())\n",
    "timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "user_input = \"who is sandeep reddy vanga\"\n",
    "thumbs_value = \"1\"\n",
    "log_entry = {\n",
    "    'log_level': 'INFO',\n",
    "    'activity_id': act_id,\n",
    "    'timestamp': timestamp,\n",
    "    'user_input': user_input,\n",
    "    'thumbs_value': thumbs_value,\n",
    "    'response': 'Bot\\'s response goes here'\n",
    "}\n",
    "logger_instance.log_info(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logger_instance.read_log_file(log_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b070799",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f533e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a838727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animal_gpt.utils.bq_class import BQ\n",
    "\n",
    "bq=  BQ()\n",
    "table = \"placeapi-333910.animalgpt.animalgpt_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84069619",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.to_bq(df=logs, table=table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adf81f",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!poetry add python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56e4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Configuration file exists at /Users/deepanshu.kandpal/Library/Application Support/pypoetry, reusing this directory.\\n\\nConsider moving TOML configuration files to /Users/deepanshu.kandpal/Library/Preferences/pypoetry, as support for the legacy directory will be removed in an upcoming release.',)\n",
      "Using version \u001b[39;1m^3.17.1\u001b[39;22m for \u001b[36mgoogle-cloud-bigquery\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(83.3s)\u001b[39;22m[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(4.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(7.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(7.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(9.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(11.2s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(11.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(13.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(13.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.2s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(14.9s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(15.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(17.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(18.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(19.8s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(21.4s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(26.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(27.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(27.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(37.8s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(40.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(41.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(42.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(44.8s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(45.8s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(47.4s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(50.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(50.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(52.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(53.8s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(55.4s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(56.2s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(56.5s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(58.4s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(60.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(60.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(64.6s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(65.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(65.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(70.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(70.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(72.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(74.7s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(79.9s)\u001b[39;22m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m2\u001b[39m installs, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m2%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m10%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m20%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m30%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m40%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m50%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m60%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m70%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m80%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m90%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m15.0.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyarrow\u001b[39m\u001b[39m (\u001b[39m\u001b[32m15.0.0\u001b[39m\u001b[39m)\u001b[39m\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdb-dtypes\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.2.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdb-dtypes\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.2.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdb-dtypes\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.2.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdb-dtypes\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m1.2.0\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mdb-dtypes\u001b[39m\u001b[39m (\u001b[39m\u001b[32m1.2.0\u001b[39m\u001b[39m)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add \"google-cloud-bigquery[pandas]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f15dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3d2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd20f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for sentences and store in Faiss index\n",
    "def create_embedding_index(sentences):\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Normalize embeddings before adding to the index\n",
    "    embeddings = np.array(embeddings)\n",
    "    embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # Create Faiss index\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Perform sentence similarity search using Faiss\n",
    "def find_similar_sentences(query, index, sentences, top_k=5):\n",
    "    query_embedding = model.encode(query)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "\n",
    "    # Perform similarity search using Faiss\n",
    "    i, similar_indices = index.search(np.array([query_embedding]).astype(np.float32), top_k)\n",
    "    print(i,similar_indices)\n",
    "    # Retrieve and return similar sentences\n",
    "    similar_sentences = [sentences[i] for i in similar_indices[0]]\n",
    "    probabilities = i[0]\n",
    "    return similar_sentences,probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\"Give me the complete cast of Animal\",\n",
    "\"Who has made Animal\",\n",
    "\"Who is the director of Animal?\",\n",
    "\"Who is Ranbir Kapoor?\",\n",
    "\"Who is Sandeep Reddy Vanga?\",\n",
    "\"Give your review of Animal\",\n",
    "\"Do you think Animal is a bad film?\",\n",
    "\"Why is Animal getting so much hate?\",\n",
    "\"is Animal a misogynist film?\",\n",
    "\"Do films carry any responsibililty for morality?\",\n",
    "\"Should filmmakers be held accountable?\",\n",
    "\"Give me your favourite scene from Animal\",\n",
    "\"Give me theories for Animal Sequel\",\n",
    "\"Tell me about Animal Sequel\",\n",
    "\"What is Animal?\",\n",
    "\"Give me Animal review?\",\n",
    "\"who is the Main hero of Animal?\",\n",
    "\"who all worked in animal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "989e96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.5423268  0.4972879  0.4949548  0.46516395]] [[ 7 14  6  1  5]]\n",
      "Query: Why is Animal getting so much hate?\n",
      "Similar Sentences: ['Why is Animal getting so much hate?', 'What is Animal?', 'Do you think Animal is a bad film?', 'Who has made Animal', 'Give your review of Animal']\n",
      "probability: [1.         0.5423268  0.4972879  0.4949548  0.46516395]\n"
     ]
    }
   ],
   "source": [
    "index = create_embedding_index(sentences)\n",
    "\n",
    "# Example query\n",
    "user_input = \"Why is Animal getting so much hate?\"\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences, probabilities = find_similar_sentences(user_input, index, sentences)\n",
    "\n",
    "# Print the results\n",
    "print(\"Query:\", user_input)\n",
    "print(\"Similar Sentences:\", similar_sentences)\n",
    "print(\"probability:\",probability )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0730f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "torch.manual_seed(100)\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./anmialGPTV1\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model_predict = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Give me your favourite scene from Animal\",], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409577cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model_predict.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0,   \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14e7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_table = \"placeapi-333910.animalgpt.animal_prompt_db\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b961d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_index =  f'''SELECT query as queries FROM `{read_table}`'''\n",
    "prompts = bq.read_bq(query = query_for_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78429d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give me the complete cast of Animal',\n",
       " 'who is the Main hero of Animal?',\n",
       " 'Do you think Animal is a bad film?',\n",
       " 'Why is Animal getting so much hate?',\n",
       " 'Who has made Animal',\n",
       " 'Who is the director of Animal?',\n",
       " 'Give your review of Animal',\n",
       " 'What is Animal?',\n",
       " 'is Animal a misogynist film?',\n",
       " 'Give me your favourite scene from Animal',\n",
       " 'Do films carry any responsibililty for morality?',\n",
       " 'Should filmmakers be held accountable?',\n",
       " 'Who is Ranbir Kapoor?',\n",
       " 'who all worked in animal',\n",
       " 'Who is Sandeep Reddy Vanga?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(prompts[\"queries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ffb12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_prompt = f'''SELECT *  FROM `{read_table}`'''\n",
    "prompt_mapping = bq.read_bq(query = query_for_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b61bcc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me the complete cast of Animal</td>\n",
       "      <td>Ranbir Kapoor in a dual role as Ranvijay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who is the Main hero of Animal?</td>\n",
       "      <td>Ranbir Kapoor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you think Animal is a bad film?</td>\n",
       "      <td>I think it's a mixed bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is Animal getting so much hate?</td>\n",
       "      <td>I know that some of you are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who has made Animal</td>\n",
       "      <td>Sandeep Reddy Vanga (pronounced),</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who is the director of Animal?</td>\n",
       "      <td>Sandeep Reddy Vanga (pronounced),</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Give your review of Animal</td>\n",
       "      <td>Critical response The film received mixed reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is Animal?</td>\n",
       "      <td>Animal is a 2023 Indian Hindi-language action ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is Animal a misogynist film?</td>\n",
       "      <td>I think so because while I was in a few minute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Give me your favourite scene from Animal</td>\n",
       "      <td>was the fight scene where Bobby Deol Abrar Haq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Do films carry any responsibililty for morality?</td>\n",
       "      <td>I think so because when we're making films whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Should filmmakers be held accountable?</td>\n",
       "      <td>I think so because when we're making films whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Who is Ranbir Kapoor?</td>\n",
       "      <td>Ranbir Kapoor (pronounced [ɾəɳˈbiːɾ kəˈpuːɾ], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>who all worked in animal</td>\n",
       "      <td>The film is produced by Bhushan Kumar, Pranay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Who is Sandeep Reddy Vanga?</td>\n",
       "      <td>Sandeep Reddy Vanga (pronounced [sˈændiːp ɹˈɛd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                Give me the complete cast of Animal   \n",
       "1                    who is the Main hero of Animal?   \n",
       "2                 Do you think Animal is a bad film?   \n",
       "3                Why is Animal getting so much hate?   \n",
       "4                                Who has made Animal   \n",
       "5                     Who is the director of Animal?   \n",
       "6                         Give your review of Animal   \n",
       "7                                    What is Animal?   \n",
       "8                       is Animal a misogynist film?   \n",
       "9           Give me your favourite scene from Animal   \n",
       "10  Do films carry any responsibililty for morality?   \n",
       "11            Should filmmakers be held accountable?   \n",
       "12                             Who is Ranbir Kapoor?   \n",
       "13                          who all worked in animal   \n",
       "14                       Who is Sandeep Reddy Vanga?   \n",
       "\n",
       "                                               prompt  \n",
       "0            Ranbir Kapoor in a dual role as Ranvijay  \n",
       "1                                       Ranbir Kapoor  \n",
       "2                            I think it's a mixed bag  \n",
       "3                         I know that some of you are  \n",
       "4                   Sandeep Reddy Vanga (pronounced),  \n",
       "5                   Sandeep Reddy Vanga (pronounced),  \n",
       "6   Critical response The film received mixed reviews  \n",
       "7   Animal is a 2023 Indian Hindi-language action ...  \n",
       "8   I think so because while I was in a few minute...  \n",
       "9   was the fight scene where Bobby Deol Abrar Haq...  \n",
       "10  I think so because when we're making films whe...  \n",
       "11  I think so because when we're making films whe...  \n",
       "12  Ranbir Kapoor (pronounced [ɾəɳˈbiːɾ kəˈpuːɾ], ...  \n",
       "13  The film is produced by Bhushan Kumar, Pranay ...  \n",
       "14  Sandeep Reddy Vanga (pronounced [sˈændiːp ɹˈɛd...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbc332c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Give me your favourite scene from Animal</td>\n",
       "      <td>was the fight scene where Bobby Deol Abrar Haq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      query  \\\n",
       "9  Give me your favourite scene from Animal   \n",
       "\n",
       "                                              prompt  \n",
       "9  was the fight scene where Bobby Deol Abrar Haq...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_mapping.query(\"query == 'Give me your favourite scene from Animal' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a41d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_embedding_index(list(prompt_mapping[\"query\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf685202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def create_prediction(query, index, sentence):\n",
    "     print(query)\n",
    "     similar_sentences, probabilities = find_similar_sentences(query, index, prompt_mapping[\"query\"])\n",
    "     print(similar_sentences)\n",
    "     prompt = prompt_mapping.query(f\"query == '{similar_sentences[0]}'\")\n",
    "     inputs = tokenizer([prompt[\"prompt\"].iloc[0],], return_tensors=\"pt\")\n",
    "     sample_output = model_predict.generate(\n",
    "     **inputs,\n",
    "     max_new_tokens=100,\n",
    "     do_sample=True,\n",
    "     top_p=0.92,\n",
    "     top_k=0,   \n",
    "     num_beams=5,\n",
    "     no_repeat_ngram_size=2,\n",
    "     num_return_sequences=5,\n",
    "     )\n",
    "     output = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45833724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Give me your favourite scene from Animal</td>\n",
       "      <td>was the fight scene where Bobby Deol Abrar Haq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      query  \\\n",
       "9  Give me your favourite scene from Animal   \n",
       "\n",
       "                                              prompt  \n",
       "9  was the fight scene where Bobby Deol Abrar Haq...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_mapping.query(\"query == 'Give me your favourite scene from Animal' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "394785d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me your favourite scene from Animal\n",
      "[[0.9999999  0.5932654  0.52839327 0.5119614  0.50351584]] [[9 0 6 1 4]]\n",
      "['Give me your favourite scene from Animal', 'Give me the complete cast of Animal', 'Give your review of Animal', 'who is the Main hero of Animal?', 'Who has made Animal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"was the fight scene where Bobby Deol Abrar Haq has just murdered someone on the day of his third wedding\\nfourth I don't even know but his face is spread with blood and he forcefully starts having sex with his newly wed wife whose pregnant by the way in front of everyone\\nonce he is done he calls for his other wives in his haram at the knife point where he says Kuchh main Tumhen yah Nahin Pata ismein main Sandeep Reddy Vanga ko game Karun ya use theatre ke crowd ko Pratiksha from our entertainment team has this\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = create_prediction(query=input, index=index,sentence=prompt_mapping)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399ffd2",
   "metadata": {},
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d686c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: service_account\n",
      "project_id: placeapi-333910\n",
      "private_key_id: cba7da27e82d1a1abf43f648bb8ffa4bf504b415\n",
      "private_key: MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDT0Lf4+t2wMr7kDQTHLaxWeUTMNCDnkk1bqwgIGcsGJU4KXtNbS5uPkhCinh/y5KuiNLFqcIxdKl3ZdVWDrNqVeNno1c78bYhmSf7lql97VOBbZU4jIgaKPnc3DuCiKQR9FP1unqT/zV5413KYmRXRpiH7qdx4yy9sGq8Z4hJhhbQCPze016b2NOkhXypBnvT1U8ujhSDC+tCTLMYR+BNMzkuN3cdwbbTPTXlij9bdHOfDpZ1C71TIWOn9h+DEsultWAPptSHBvup22pl2c4MK2OmUSrLZxtp2hp7wYxNYhQpz29EcaOC9NdKjP2vcZucdbbIRTzewhsjMLTQC22LBAgMBAAECggEAC1TVIrq49V4hnC2eHdGxsIaYSZxYTksjXE4QBe7rT47bV+6uu+mVjt6viSOQbfwRuoG7JAiVWb6UereuNXcf0nM+pRgbFKH/dAltW3f716s4vWrz/Skq1GZ3Fjjzh3lORirDTBGsngqsjSZpeQRniGms07vIkaW9Cbl7uJTBivn1NdyiCVnLyRgnJ/cBcK9Ic0TPM8fi2AWPBNkeFFK3QNG37EzrTWuY0ygtTNs5jR0FbTsBru6FQz01AOeRSbrOvH7kgN3JsqTOCBj6YckUNuZSLltYi3Y+aKkpTg9eMLHDDD2vL7Bw8f7d4aZ8jUDHz0rxUX7PC1Qebwx2M6jkaQKBgQDtdmFcN//V0kWGOlrV+MEMz+EWdakuuTTuJYHrKk2F58O6sinDI1PkBqb5VuneUi4JC3QOvUtS1boTF0xjynOMbDPcf1EusIbNziGrefNpTfUlaP3Kj7elJmSi8EX4AReseVEnOvjISC8TAk3vWYkOGxeKn4UL3e88mU7FrmiucwKBgQDkWcm2P/VhEPR6spQSTJxmbTSbudPeLyDvP/LIZjiHllSNNLkoM3qefhlliaN1YNVGPX4DNXPNPxvT+HHyAM/nfzJkedlSGtIhRC0jhosNnlRH1/LiPixHWviDSTpxD0oLnVFQQrmBR1S9i+Mo/3vHxYfoU0JRosDknGMV+DhI+wKBgFAEiCJPLIKc7JiiqWFZBZSPqEWtOGlqN4/YS8M26hCpXKdYygHbsbUxGKsEwqIMBz89AMrGts8e/ijYvQETg4RaIc+3h1+4VHSddXMXERL3FhiZHIxZiYS5P0g4txsvIw8XgmU51gOHb3V0XVa+URTTxEcOIgMxsiA1ePncH6JJAoGBAKDpfLEG5P0fm4gS6zDx94xeDCJokpoRD95Ia+c30UIKd06CBrADhrbFGVRp51vBfsjecDwGoLHTdOKsQm9MjzUMdxBCpResesKibkSIfNh959ownKE9OqqVumWZZqthIMaKd1BdlKc7AsYHG246ipk94wWeFi+8xrzSTyieH9udAoGBANT8PlyWFRIGiQSKUwpw9GrfeeKfsLEorB9YsSLPa+lLIzpBmaeSPRMCAhT/vJ+BHnlJ5kXcqU8oMvDk62DNBLU8/+kURbLR9iDOnDyoavw+DLhB3FHInqh1pYZe4xpFVBD5c8m4aEJVcPdqWb+dDJ43dsgLaXNZw2w34CjXxy2w\n",
      "client_email: soai-dev-acn@placeapi-333910.iam.gserviceaccount.com\n",
      "client_id: 115497279223632701890\n",
      "auth_uri: https://accounts.google.com/o/oauth2/auth\n",
      "token_uri: https://oauth2.googleapis.com/token\n",
      "auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n",
      "client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/soai-dev-acn%40placeapi-333910.iam.gserviceaccount.com\n",
      "universe_domain: googleapis.com\n",
      "prompt_read_table: placeapi-333910.animalgpt.animal_prompt_db\n",
      "default_logs_db: placeapi-333910.animalgpt.animalgpt_logs\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ.clear()\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"animal.env\")\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50caece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animal_gpt.predict.core import Prediction\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"animal.env\")\n",
    "model_path = \"dkandpalz/animalGPT1\"\n",
    "\n",
    "predict = Prediction(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e548587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import uuid\n",
    "from  animal_gpt.utils.bq_class import BQ\n",
    "from  animal_gpt.utils.config import (\n",
    "    default_logs_db\n",
    ")\n",
    "from animal_gpt.utils.logger import log\n",
    "\n",
    "\n",
    "@log\n",
    "def generate_prediction(user_input):\n",
    "    act_id = str(uuid.uuid4())\n",
    "    output, probabilities, predicted_prompt = predict.create_prediction(user_input)\n",
    "    thumbs_value = 'fantastic'\n",
    "    return output, probabilities, thumbs_value ,predicted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ad3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"tell me about animal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853b912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Critical response The film received mixed reviews from critics and audiences and broke several box office records for a Hindi film, including the highest non-holiday opening, biggest single days, highest opening weekend and highest open week in India. At the overseas box-office, it broke the previous record held by Ranbir Kapoor's Brahmāstra: Part One – Shiva () and Jawan ().[][] In the first week it earned a total worldwide gross collection of ₹. crore (US$. million). On its th day\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = generate_prediction(input)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6e4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
